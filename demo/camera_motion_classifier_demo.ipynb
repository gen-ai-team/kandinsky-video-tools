{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dcd62a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:37:09.418319: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-23 18:37:09.430928: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753285029.444432   74181 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753285029.448453   74181 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753285029.460479   74181 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753285029.460498   74181 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753285029.460500   74181 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753285029.460502   74181 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-23 18:37:09.464504: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "import torch\n",
    "import decord \n",
    "import numpy as np\n",
    "from IPython.display import Video\n",
    "from transformers import (\n",
    "    VideoMAEImageProcessor, VideoMAEForVideoClassification\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e77b4248",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = torch.bfloat16\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c2e22e",
   "metadata": {},
   "source": [
    "## Model Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41937514",
   "metadata": {},
   "source": [
    "The specific model used here is a [VideoMAE model](https://huggingface.co/docs/transformers/model_doc/videomae)(`large`) variant that has been finetuned for multi-label video classification (a video can belong to multiple classes simultaneously)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23a0273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and processor\n",
    "model_ckpt = \"ai-forever/kandinsky-videomae-large-camera-motion\"\n",
    "image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\n",
    "\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    attn_implementation=\"sdpa\",\n",
    "    torch_dtype=torch_dtype,\n",
    ").eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31b24ef",
   "metadata": {},
   "source": [
    "### Model Classes and Their Meanings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f77606",
   "metadata": {},
   "source": [
    "The model predicts `21` different camera motion and shot type classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad5b9523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'arc_left',\n",
       " 1: 'arc_right',\n",
       " 2: 'dolly_in',\n",
       " 3: 'dolly_out',\n",
       " 4: 'pan_left',\n",
       " 5: 'pan_right',\n",
       " 6: 'pedestal_down',\n",
       " 7: 'pedestal_up',\n",
       " 8: 'pov',\n",
       " 9: 'roll_left',\n",
       " 10: 'roll_right',\n",
       " 11: 'shake',\n",
       " 12: 'static',\n",
       " 13: 'tilt_down',\n",
       " 14: 'tilt_up',\n",
       " 15: 'track',\n",
       " 16: 'truck_left',\n",
       " 17: 'truck_right',\n",
       " 18: 'undefined',\n",
       " 19: 'zoom_in',\n",
       " 20: 'zoom_out'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aa775e",
   "metadata": {},
   "source": [
    "##### Horizontal Movements\n",
    "1. `arc_left`/`arc_right` – Camera moves in a leftward/rightward arc around the subject.\n",
    "\n",
    "2. `pan_left`/`pan_right` – Camera rotates horizontally to the left/to the right (fixed position).\n",
    "\n",
    "3. `truck_left`/`truck_right` – Camera slides left/right (lateral movement, keeping axis perpendicular).\n",
    "\n",
    "##### Vertical Movements\n",
    "4. `pedestal_down`/`pedestal_up` – Camera moves vertically downward/upward (e.g., lowering/raising a tripod).\n",
    "\n",
    "5. `tilt_down`/`tilt_up` – Camera tilts downward/upward (angle change, fixed position).\n",
    "\n",
    "##### Forward/Backward Movements\n",
    "6. `dolly_in`/`dolly_out` – Camera moves physically forward/away.\n",
    "\n",
    "7. `zoom_in`/`zoom_out` – Optical zoom-in/out (lens adjustment, no physical movement).\n",
    "\n",
    "#####  Rotational  Movements\n",
    "8. `roll_left`/`roll_right` – Camera rolls left/right (rotates on its axis).\n",
    "\n",
    "##### Special Shots\n",
    "\n",
    "9. `shake` – Shaky/unstable movement (handheld or intentional effect).\n",
    "\n",
    "10. `track` – Camera follows a moving subject (e.g., on rails or steadycam).\n",
    "\n",
    "11. `pov` – Point-of-view shot (camera mimics a character’s perspective).\n",
    "\n",
    "12. `static` – Fixed shot (no camera movement).\n",
    "\n",
    "##### Other\n",
    "13. `undefined` – Unclassifiable or ambiguous motion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e457b247",
   "metadata": {},
   "source": [
    "### Key Features\n",
    "\n",
    "(a) Multi-label classification: A single video can belong to multiple classes (e.g., `dolly_in` + `pan_right` + `track` + `shake`).\n",
    "\n",
    "(b) Model was trained to associate entire video with camera labels, not frame-level motions(!): [input video] -> label/labels (because multilabel) for all video.  So, if this camera motion exists during all video frames model should predict this motion, otherwise it should predict `undefined`.\n",
    "\n",
    "\n",
    "(c) Predictions use `sigmoid` with a `0.5` cutoff for activation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cfdd5c",
   "metadata": {},
   "source": [
    "### Technical Notes\n",
    "\n",
    "1) Input frames are resized to `224x224` pixels.\n",
    "\n",
    "\n",
    "2) The model is configured to process `config.num_frames=16` frames per input clip. These frames are extracted uniformly from the input video, regardless of its original duration. \n",
    "\n",
    "Here the video is loaded using `decord.VideoReader`, which efficiently decodes frames without reading the entire file into memory.\n",
    "For any input video, we sample exactly `config.num_frames=16` frames, spaced evenly across its duration.\n",
    "\n",
    "However, for videos longer than **2 seconds**, processing the entire video as a single clip may miss temporal nuances (e.g., varying camera motions). So, recommended workflow for such videos will be follows:\n",
    "\n",
    " (a) split the video into non-overlapping 2-second segments (or sliding windows with optional overlap).\n",
    "\n",
    " (b) run inference independently on each segment. \n",
    "\n",
    " (c) post-process results.\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0366e119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "num_frames = model.config.num_frames\n",
    "height, width = image_processor.crop_size['height'], image_processor.crop_size['width']\n",
    "\n",
    "def load_video(filepath, num_frames=16, clip_start=0, clip_end=-1):\n",
    "    \"\"\"Load a video and select num_frames frames.\n",
    "    This function loads a video file and extracts a specified number of frames (num_frames),\n",
    "    evenly distributed over the time interval defined by the clip_start and clip_end parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to the video file.\n",
    "    num_frames : int, optional\n",
    "        Number of frames to extract.\n",
    "    clip_start : float, optional\n",
    "        Start time of the clip in seconds. Default is 0.\n",
    "    clip_end : float, optional\n",
    "        End time of the clip in seconds. If set to -1, the clip continues until the end of the video. Default is -1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Tensor containing video frames in the format (C, T, H, W).\n",
    "    \"\"\"\n",
    "    vr = decord.VideoReader(\n",
    "        filepath,\n",
    "        num_threads=1,\n",
    "        # ctx=decord.cpu(0),\n",
    "        width=width,\n",
    "        height=height,\n",
    "    )\n",
    "    total_frames = len(vr)\n",
    "    fps = float(vr.get_avg_fps())\n",
    "    duration = float(total_frames) / fps\n",
    "\n",
    "    start_idx = math.ceil(fps * clip_start)\n",
    "    if clip_end != -1 and clip_end < duration:\n",
    "        end_idx = math.ceil(fps * clip_end)\n",
    "    else:\n",
    "        end_idx = total_frames - 1\n",
    "\n",
    "    frame_indices = np.linspace(start_idx, end_idx, num=num_frames, dtype=int)\n",
    "    \n",
    "    video = vr.get_batch(frame_indices).asnumpy()  # (T, H, W, C)\n",
    "    video = torch.from_numpy(video).permute(3, 0, 1, 2)  # (C, T, H, W)\n",
    "    \n",
    "    return video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486ac871",
   "metadata": {},
   "source": [
    "\n",
    "Need perform the normalization and resizing of the input video tensor, followed by normalization using mean and standard deviation values from an image processor. The final tensor is permuted to match the expected format for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dd641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_video(video, device='cuda'):\n",
    "    \"\"\"Apply transformations to the video.\n",
    "    \"\"\"\n",
    "    video = video.to(device) / 255.0\n",
    "    video = torch.nn.functional.interpolate(\n",
    "        video, size=(height, width), mode=\"bilinear\"\n",
    "    )\n",
    "    mean = torch.tensor(image_processor.image_mean).view(3, 1, 1, 1).to(device)\n",
    "    std = torch.tensor(image_processor.image_std).view(3, 1, 1, 1).to(device)\n",
    "\n",
    "    video = (video - mean) / std\n",
    "    video = video.permute(1, 0, 2, 3)\n",
    "    return video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c45dc9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(model, filepath, clip_start=0, clip_end=-1):\n",
    "    video = load_video(filepath, clip_start=clip_start, clip_end=clip_end)\n",
    "\n",
    "    inputs = preprocess_video(video).unsqueeze(0).to(torch_dtype)\n",
    "    print(inputs.shape)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "\n",
    "    logits = outputs.logits.float()\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()[0]  # multi-label\n",
    "\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    predicted_labels = [model.config.id2label[i] for i, p in enumerate(preds) if p == 1]\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c24c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-23 18:37:36--  https://huggingface.co/datasets/syCen/CameraBench/resolve/main/videos/015a2bdd4aa4b5cfb60aacde6a3069ea3e4ef458c6f263148b2339b546ef8e86.1.mp4\n",
      "Resolving huggingface.co (huggingface.co)... 3.164.240.65, 3.164.240.43, 3.164.240.18, ...\n",
      "Connecting to huggingface.co (huggingface.co)|3.164.240.65|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.hf.co/repos/05/c5/05c58eb35dc06dce7c833d554b8d8ec7c755e9e81124e7ec1c6a3c468e929cd2/48224284d16cce043e2737abc6f2a2abc7e4952aa2e0ae2a5ecfdcbb4241b48a?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27015a2bdd4aa4b5cfb60aacde6a3069ea3e4ef458c6f263148b2339b546ef8e86.1.mp4%3B+filename%3D%22015a2bdd4aa4b5cfb60aacde6a3069ea3e4ef458c6f263148b2339b546ef8e86.1.mp4%22%3B&response-content-type=video%2Fmp4&Expires=1753288657&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzI4ODY1N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzA1L2M1LzA1YzU4ZWIzNWRjMDZkY2U3YzgzM2Q1NTRiOGQ4ZWM3Yzc1NWU5ZTgxMTI0ZTdlYzFjNmEzYzQ2OGU5MjljZDIvNDgyMjQyODRkMTZjY2UwNDNlMjczN2FiYzZmMmEyYWJjN2U0OTUyYWEyZTBhZTJhNWVjZmRjYmI0MjQxYjQ4YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=pBeuM00e6wRsnDPLqPddmL%7EmF76l%7EU5eaeD2Zl9txLGB3pKx8EAAce7gw3iiPLzYd0R8a7TH3Tz8r%7EXQDwLHDRPkmdVYhatsG8I2%7E9XheYhsunHJWDnbyOvWTiN0oagjwsNg1HsN5HRyRUj4ZZjemxJHeSHOR2GEjNsRm8avw%7E%7Ev4-d%7E86NujD-xInah7HhCK8d-JZvtcN7lHEjw9UXYoCAj0Z5KUlYKUQi7rvUIK8ZBW%7EsgXy1Wh355%7E0hOcnX96XkE-m7W0iZcbztgeicAtxGBGgdsEvi2tmHZRSmS65jdAutdoI-dULqjkT89LHz%7Elyp%7EYk-OGKJpCLe9YkXv2g__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
      "--2025-07-23 18:37:37--  https://cdn-lfs-us-1.hf.co/repos/05/c5/05c58eb35dc06dce7c833d554b8d8ec7c755e9e81124e7ec1c6a3c468e929cd2/48224284d16cce043e2737abc6f2a2abc7e4952aa2e0ae2a5ecfdcbb4241b48a?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27015a2bdd4aa4b5cfb60aacde6a3069ea3e4ef458c6f263148b2339b546ef8e86.1.mp4%3B+filename%3D%22015a2bdd4aa4b5cfb60aacde6a3069ea3e4ef458c6f263148b2339b546ef8e86.1.mp4%22%3B&response-content-type=video%2Fmp4&Expires=1753288657&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzI4ODY1N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzA1L2M1LzA1YzU4ZWIzNWRjMDZkY2U3YzgzM2Q1NTRiOGQ4ZWM3Yzc1NWU5ZTgxMTI0ZTdlYzFjNmEzYzQ2OGU5MjljZDIvNDgyMjQyODRkMTZjY2UwNDNlMjczN2FiYzZmMmEyYWJjN2U0OTUyYWEyZTBhZTJhNWVjZmRjYmI0MjQxYjQ4YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=pBeuM00e6wRsnDPLqPddmL%7EmF76l%7EU5eaeD2Zl9txLGB3pKx8EAAce7gw3iiPLzYd0R8a7TH3Tz8r%7EXQDwLHDRPkmdVYhatsG8I2%7E9XheYhsunHJWDnbyOvWTiN0oagjwsNg1HsN5HRyRUj4ZZjemxJHeSHOR2GEjNsRm8avw%7E%7Ev4-d%7E86NujD-xInah7HhCK8d-JZvtcN7lHEjw9UXYoCAj0Z5KUlYKUQi7rvUIK8ZBW%7EsgXy1Wh355%7E0hOcnX96XkE-m7W0iZcbztgeicAtxGBGgdsEvi2tmHZRSmS65jdAutdoI-dULqjkT89LHz%7Elyp%7EYk-OGKJpCLe9YkXv2g__&Key-Pair-Id=K24J24Z295AEI9\n",
      "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.164.68.118, 3.164.68.54, 3.164.68.112, ...\n",
      "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.164.68.118|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5460796 (5.2M) [video/mp4]\n",
      "Saving to: ‘015a2bdd4aa4b5cfb60aacde6a3069ea3e4ef458c6f263148b2339b546ef8e86.1.mp4’\n",
      "\n",
      "015a2bdd4aa4b5cfb60 100%[===================>]   5.21M  14.3MB/s    in 0.4s    \n",
      "\n",
      "2025-07-23 18:37:39 (14.3 MB/s) - ‘015a2bdd4aa4b5cfb60aacde6a3069ea3e4ef458c6f263148b2339b546ef8e86.1.mp4’ saved [5460796/5460796]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://huggingface.co/datasets/syCen/CameraBench/resolve/main/videos/015a2bdd4aa4b5cfb60aacde6a3069ea3e4ef458c6f263148b2339b546ef8e86.1.mp4\" -O \"examples/camera_motion/015a2bdd4aa4b5cfb60aacde6a3069ea3e4ef458c6f263148b2339b546ef8e86.1.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fac83a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"examples/camera_motion/015a2bdd4aa4b5cfb60aacde6a3069ea3e4ef458c6f263148b2339b546ef8e86.1.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf460bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 3, 224, 224])\n",
      "Predicted labels: ['arc_right']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"015a2bdd4aa4b5cfb60aacde6a3069ea3e4ef458c6f263148b2339b546ef8e86.1.mp4\" controls  width=\"512\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = predict_labels(model, filepath)\n",
    "\n",
    "print(f\"Predicted labels: {predicted_labels}\")\n",
    "Video(filepath, width=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a2c129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-23 18:37:42--  https://huggingface.co/datasets/syCen/CameraBench/resolve/main/videos/0Um7WnY72Us.1.0.mp4\n",
      "Resolving huggingface.co (huggingface.co)... 3.164.240.65, 3.164.240.43, 3.164.240.18, ...\n",
      "Connecting to huggingface.co (huggingface.co)|3.164.240.65|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.hf.co/repos/05/c5/05c58eb35dc06dce7c833d554b8d8ec7c755e9e81124e7ec1c6a3c468e929cd2/3db1df636d555126d686fcabc9bb84fef02d87ddc1c03d7df3325a9db648aafb?response-content-disposition=inline%3B+filename*%3DUTF-8%27%270Um7WnY72Us.1.0.mp4%3B+filename%3D%220Um7WnY72Us.1.0.mp4%22%3B&response-content-type=video%2Fmp4&Expires=1753288662&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzI4ODY2Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzA1L2M1LzA1YzU4ZWIzNWRjMDZkY2U3YzgzM2Q1NTRiOGQ4ZWM3Yzc1NWU5ZTgxMTI0ZTdlYzFjNmEzYzQ2OGU5MjljZDIvM2RiMWRmNjM2ZDU1NTEyNmQ2ODZmY2FiYzliYjg0ZmVmMDJkODdkZGMxYzAzZDdkZjMzMjVhOWRiNjQ4YWFmYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=ViLArwjs4tMsIX-Ta-a3qL5z-E6mHag7ODQ3yoUeQpYtQiJIcQdiqW80C50nrSoUdg%7E0GgZxbB1e8n0niSDPAKEFKpej8-vQ49wEmEzlzo3%7En3r67jH7Pu1epVpYCYAv9HPXqQDa9wnJ1IXCC0dsbJc0E9RSZ8d0hws%7E%7ExFuEKfFNdGjfHn08OGbRJ0j3yGYpt-agKGsupfMz7kjb9twAVsTO26HXN0SVdpl%7EytUFA0CyG0YKQ1Xc4Q9pAagqYhA1XvdPvv68J7Y5UMtrVv96h%7Ec%7E5uRu7YMSVmeW1yW8wwiahdvHAvPq-WuFoHmjd4c2hwfT5aQisw6qbuoYoZu-A__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
      "--2025-07-23 18:37:42--  https://cdn-lfs-us-1.hf.co/repos/05/c5/05c58eb35dc06dce7c833d554b8d8ec7c755e9e81124e7ec1c6a3c468e929cd2/3db1df636d555126d686fcabc9bb84fef02d87ddc1c03d7df3325a9db648aafb?response-content-disposition=inline%3B+filename*%3DUTF-8%27%270Um7WnY72Us.1.0.mp4%3B+filename%3D%220Um7WnY72Us.1.0.mp4%22%3B&response-content-type=video%2Fmp4&Expires=1753288662&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzI4ODY2Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzA1L2M1LzA1YzU4ZWIzNWRjMDZkY2U3YzgzM2Q1NTRiOGQ4ZWM3Yzc1NWU5ZTgxMTI0ZTdlYzFjNmEzYzQ2OGU5MjljZDIvM2RiMWRmNjM2ZDU1NTEyNmQ2ODZmY2FiYzliYjg0ZmVmMDJkODdkZGMxYzAzZDdkZjMzMjVhOWRiNjQ4YWFmYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=ViLArwjs4tMsIX-Ta-a3qL5z-E6mHag7ODQ3yoUeQpYtQiJIcQdiqW80C50nrSoUdg%7E0GgZxbB1e8n0niSDPAKEFKpej8-vQ49wEmEzlzo3%7En3r67jH7Pu1epVpYCYAv9HPXqQDa9wnJ1IXCC0dsbJc0E9RSZ8d0hws%7E%7ExFuEKfFNdGjfHn08OGbRJ0j3yGYpt-agKGsupfMz7kjb9twAVsTO26HXN0SVdpl%7EytUFA0CyG0YKQ1Xc4Q9pAagqYhA1XvdPvv68J7Y5UMtrVv96h%7Ec%7E5uRu7YMSVmeW1yW8wwiahdvHAvPq-WuFoHmjd4c2hwfT5aQisw6qbuoYoZu-A__&Key-Pair-Id=K24J24Z295AEI9\n",
      "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.164.68.118, 3.164.68.54, 3.164.68.112, ...\n",
      "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.164.68.118|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 438962 (429K) [video/mp4]\n",
      "Saving to: ‘0Um7WnY72Us.1.0.mp4’\n",
      "\n",
      "0Um7WnY72Us.1.0.mp4 100%[===================>] 428.67K  1.97MB/s    in 0.2s    \n",
      "\n",
      "2025-07-23 18:37:43 (1.97 MB/s) - ‘0Um7WnY72Us.1.0.mp4’ saved [438962/438962]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://huggingface.co/datasets/syCen/CameraBench/resolve/main/videos/0Um7WnY72Us.1.0.mp4\"  -O \"examples/camera_motion/0Um7WnY72Us.1.0.mp4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c908ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"examples/camera_motion/0Um7WnY72Us.1.0.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d628540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 3, 224, 224])\n",
      "Predicted labels: ['shake', 'undefined']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"0Um7WnY72Us.1.0.mp4\" controls  width=\"512\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = predict_labels(model, filepath, clip_start=0, clip_end=-1)\n",
    "\n",
    "print(f\"Predicted labels: {predicted_labels}\")\n",
    "Video(filepath, width=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b352ac42",
   "metadata": {},
   "source": [
    "It's really undefined class by definition... Let's try to predict for segment from 1s..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df05e7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 3, 224, 224])\n",
      "Predicted labels: ['tilt_up']\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = predict_labels(model, filepath, clip_start=1, clip_end=-1)\n",
    "\n",
    "print(f\"Predicted labels: {predicted_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7837a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7746d938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmitrienko_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
